{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LSTM to perform sentiment analysis on twitter streams***\n",
    "\n",
    "London South East (Bulletin Board)\n",
    "Motley Fool (Bulletin Board) \n",
    "Investors Chronicle\n",
    "Share Prophets (Blog) \n",
    "Interactive Investor (Bulletin Board)\n",
    "ADVFN (Bulletin Board)\n",
    "MoneyAM\n",
    "Morning Star\n",
    "Guerilla Investing (Blog) (Daniel Levi)\n",
    "Tom Winnifrith (Blog)\n",
    "https://www.discussthemarket.com/streams \n",
    "Twitter â€“Ben Turney, Chris Oil, Tom Winnifrith, Daniel Levi, Zak Mir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "London South East\n",
    "http://www.lse.co.uk/\n",
    "\n",
    "\"Latest Finance & Stock Market News\n",
    "London close: Stocks end little changed despite weak reading on industry\n",
    "[Today 17:02]\n",
    "(ShareCast News) - London stocks reversed an earlier small loss to trade slightly higher by the close of trading on Monday despite resilience in the pound against the greenback even after the release of weaker-than-expected data on industrial trends.\"\n",
    "\n",
    "Motley Fool\n",
    "https://www.fool.com/investing/2017/10/22/3-top-oil-stocks-for-october.aspx\n",
    "\n",
    "3 Top Oil Stocks for October\n",
    "Here's why you might want to consider Murphy Oil, EOG Resources, and Helmerich & Payne for your portfolio.\n",
    "\n",
    "Tyler Crowe, Matthew DiLallo, And Sean O'Reilly (TMFDirtyBird) Oct 22, 2017 at 7:32AM\n",
    "It's still quite surprising that even more than three years since oil and gas stocks started their 2014 tumble, companies in this industry are still quite cheap. With valuations in so many other industries at or near all-time highs, it looks like oil and gas is one of the few places left to find value investments. \n",
    "\n",
    "So, we asked three of our investing contributors to each pitch a stock in the oil and gas industry that looks like a good investment this month. Here's a rundown of their picks, Murphy Oil (NYSE:MUR), EOG Resources (NYSE:EOG), and Helmerich & Payne (NYSE:HP). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Pre-processing string data\"\"\"\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from six.moves import xrange\n",
    "import six.moves.cPickle as pickle\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "\n",
    "\n",
    "def prepare_data(seqs, labels, maxlen=None):\n",
    "    \"\"\"Create the matrices from the datasets.\n",
    "\n",
    "    This pad each sequence to the same lenght: the lenght of the\n",
    "    longuest sequence or maxlen.\n",
    "\n",
    "    if maxlen is set, we will cut all sequence to this maximum\n",
    "    lenght.\n",
    "\n",
    "    This swap the axis!\n",
    "    \"\"\"\n",
    "    # x: a list of sentences\n",
    "    lengths = [len(s) for s in seqs]\n",
    "\n",
    "    if maxlen is not None:\n",
    "        new_seqs = []\n",
    "        new_labels = []\n",
    "        new_lengths = []\n",
    "        for l, s, y in zip(lengths, seqs, labels):\n",
    "            if l < maxlen:\n",
    "                new_seqs.append(s)\n",
    "                new_labels.append(y)\n",
    "                new_lengths.append(l)\n",
    "        lengths = new_lengths\n",
    "        labels = new_labels\n",
    "        seqs = new_seqs\n",
    "\n",
    "        if len(lengths) < 1:\n",
    "            return None, None, None\n",
    "\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = numpy.max(lengths)\n",
    "\n",
    "    x = numpy.zeros((maxlen, n_samples)).astype('int64')\n",
    "    x_mask = numpy.zeros((maxlen, n_samples)).astype(theano.config.floatX)\n",
    "    for idx, s in enumerate(seqs):\n",
    "        x[:lengths[idx], idx] = s\n",
    "        x_mask[:lengths[idx], idx] = 1.\n",
    "\n",
    "    return x, x_mask, labels\n",
    "\n",
    "\n",
    "def get_dataset_file(dataset, default_dataset, origin):\n",
    "    '''Look for it as if it was a full path, if not, try local file,\n",
    "    if not try in the data directory.\n",
    "\n",
    "    Download dataset if it is not present\n",
    "\n",
    "    '''\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the data directory.\n",
    "        new_path = os.path.join(\n",
    "            os.path.split(__file__)[0],\n",
    "            \"..\",\n",
    "            \"data\",\n",
    "            dataset\n",
    "        )\n",
    "        if os.path.isfile(new_path) or data_file == default_dataset:\n",
    "            dataset = new_path\n",
    "\n",
    "    if (not os.path.isfile(dataset)) and data_file == default_dataset:\n",
    "        from six.moves import urllib\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, dataset)\n",
    "\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " def load_data(path=\"imdb.pkl\", n_words=100000, valid_portion=0.1, maxlen=None,\n",
    "              sort_by_len=True):\n",
    "    '''Loads the dataset\n",
    "\n",
    "    :type path: String\n",
    "    :param path: The path to the dataset (here IMDB)\n",
    "    :type n_words: int\n",
    "    :param n_words: The number of word to keep in the vocabulary.\n",
    "        All extra words are set to unknow (1).\n",
    "    :type valid_portion: float\n",
    "    :param valid_portion: The proportion of the full train set used for\n",
    "        the validation set.\n",
    "    :type maxlen: None or positive int\n",
    "    :param maxlen: the max sequence length we use in the train/valid set.\n",
    "    :type sort_by_len: bool\n",
    "    :name sort_by_len: Sort by the sequence lenght for the train,\n",
    "        valid and test set. This allow faster execution as it cause\n",
    "        less padding per minibatch. Another mechanism must be used to\n",
    "        shuffle the train set at each epoch.\n",
    "    '''\n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    "\n",
    "    # Load the dataset\n",
    "    path = get_dataset_file(\n",
    "        path, \"imdb.pkl\",\n",
    "        \"http://www.iro.umontreal.ca/~lisa/deep/data/imdb.pkl\")\n",
    "\n",
    "    if path.endswith(\".gz\"):\n",
    "        f = gzip.open(path, 'rb')\n",
    "    else:\n",
    "        f = open(path, 'rb')\n",
    "\n",
    "    train_set = pickle.load(f)\n",
    "    test_set = pickle.load(f)\n",
    "    f.close()\n",
    "    if maxlen:\n",
    "        new_train_set_x = []\n",
    "        new_train_set_y = []\n",
    "        for x, y in zip(train_set[0], train_set[1]):\n",
    "            if len(x) < maxlen:\n",
    "                new_train_set_x.append(x)\n",
    "                new_train_set_y.append(y)\n",
    "        train_set = (new_train_set_x, new_train_set_y)\n",
    "        del new_train_set_x, new_train_set_y\n",
    "\n",
    "    # split training set into validation set\n",
    "    train_set_x, train_set_y = train_set\n",
    "    n_samples = len(train_set_x)\n",
    "    sidx = numpy.random.permutation(n_samples)\n",
    "    n_train = int(numpy.round(n_samples * (1. - valid_portion)))\n",
    "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
    "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
    "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
    "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
    "\n",
    "    train_set = (train_set_x, train_set_y)\n",
    "    valid_set = (valid_set_x, valid_set_y)\n",
    "\n",
    "    def remove_unk(x):\n",
    "        return [[1 if w >= n_words else w for w in sen] for sen in x]\n",
    "\n",
    "    test_set_x, test_set_y = test_set\n",
    "    valid_set_x, valid_set_y = valid_set\n",
    "    train_set_x, train_set_y = train_set\n",
    "\n",
    "    train_set_x = remove_unk(train_set_x)\n",
    "    valid_set_x = remove_unk(valid_set_x)\n",
    "    test_set_x = remove_unk(test_set_x)\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(test_set_x)\n",
    "        test_set_x = [test_set_x[i] for i in sorted_index]\n",
    "        test_set_y = [test_set_y[i] for i in sorted_index]\n",
    "\n",
    "        sorted_index = len_argsort(valid_set_x)\n",
    "        valid_set_x = [valid_set_x[i] for i in sorted_index]\n",
    "        valid_set_y = [valid_set_y[i] for i in sorted_index]\n",
    "\n",
    "        sorted_index = len_argsort(train_set_x)\n",
    "        train_set_x = [train_set_x[i] for i in sorted_index]\n",
    "        train_set_y = [train_set_y[i] for i in sorted_index]\n",
    "\n",
    "    train = (train_set_x, train_set_y)\n",
    "    valid = (valid_set_x, valid_set_y)\n",
    "    test = (test_set_x, test_set_y)\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named imdb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7837da832513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng_mrg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMRG_RandomStreams\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mRandomStreams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named imdb"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Build a tweet sentiment analyzer\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import six.moves.cPickle as pickle\n",
    "\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "from theano import config\n",
    "import theano.tensor as tensor\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
